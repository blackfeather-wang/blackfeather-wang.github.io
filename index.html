
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="description" content="Yulin Wang's homepage">
	<link rel="stylesheet" href="./files/jemdoc.css" type="text/css">
	<title>Yulin Wang (王语霖)</title>
   <link rel="icon" href="files/icon.jpg" >
   <style>
  .news-scroller{
    max-height: 350px;          /* 需要更高/更低就调这里 */
    overflow-y: auto;
    padding: 10px 14px;
    border: 0px solid #fff;  /* 红框 */
    border-radius: 0px;
    background: #fff;
    -webkit-overflow-scrolling: touch; /* 移动端更顺滑 */
   }
   @media (max-width: 800px){
      .news-scroller{ max-height: 260px; }
   }
   </style>
</head>


<body>

<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Yulin Wang (王语霖) &nbsp;</div>

				<h3>Ph.D.</h3>  
				<p>
					Department of Automation, Tsinghua University<br>
					<!-- Beijing, China, 100084<br> -->
					<br>
Email:  <br> <a href="mailto:yulin-wang@tsinghua.edu.cn">yulin-wang@tsinghua.edu.cn</a> <br>  <a href="mailto:yulin.bh@gmail.com">yulin.bh@gmail.com</a> <br>
<!-- <a href="files/wyl_cv.pdf">[Curriculum Vitae (12/2024)]</a>   -->
<br>
<a href="https://scholar.google.com/citations?user=gBP38gcAAAAJ&hl=en">[Google Scholar]</a>
<a href="https://orcid.org/0000-0002-1363-0234">[ORCID]</a>
<a href="https://www.semanticscholar.org/author/Yulin-Wang/39041697">[Semantic Scholar]</a>
<a href="https://www.scopus.com/authid/detail.uri?authorId=57215223652">[Scopus]</a>
					<br>
				</p>
			</td>
			<td>
				<img src="./files/Yulin_Wang.jpg" border="0" width="200">
			</td>
		</tr><tr>
	</tr></tbody>
</table>
<h2>Short Bio</h2>
<p>
	Yulin Wang received his Ph.D. degree in the <a href="http://www.au.tsinghua.edu.cn/">Department of Automation</a> at <a href="https://www.tsinghua.edu.cn/en/index.htm">Tsinghua University</a> in 2025, advised by Prof. <a href="http://www.gaohuang.net/">Gao Huang</a> and Prof. <a href="https://www.tsinghua.edu.cn/info/1166/93894.htm">Cheng Wu</a>.
	Before that, he received his B.E. degree in <a href="http://dept3.buaa.edu.cn/">Automation</a> at <a href="https://www.buaa.edu.cn/">Beihang University</a> in 2019.
	He was a research intern at <a href="https://deepdrive.berkeley.edu/">Berkeley DeepDrive, U.C. Berkeley</a>, in 2018. 
   <br>
   <br>
   His research focuses on addressing the challenges of computational efficiency and data efficiency in building large-scale AI foundation models (e.g., visual/multi-modal foundation models, generative models, and embodied foundation models for robotics). 
   He is also interested in developing new AI-based methods for addressing challenging scientific problems (e.g., medicine and geophysics), especially from the perspective of vision-language foundation models.
   <br>
   <!-- <br>
   <b><font color="red">He is actively looking for a postdoc position (expected to graduate in 2025.06)!</font></b>
   <br> -->
   <br> 

<h2>News</h2>

<ul>
<div class="news-scroller">
   <li> [2025.09] Our newest work on dynamic neural networks, <a href="">AdaptiveNN</a>, is Accepted by <b><font color="red">Nature Machine Intelligence</font> (5-year IF=31.8)</b>. [<a href="https://github.com/LeapLabTHU/AdaptiveNN">GitHub</a>] 
   </li>
   <li> [2025.09] Two Papers are Accepted by NeurIPS 2025 (with one <b><font color="red">spotlight</font></b>).
   </li>
   <li> [2025.07] <a href="https://www.nature.com/articles/s41467-025-62865-w">UltraBot</a> is Accepted by <b><font color="red">Nature Communications</font> (5-year IF=17.2)</b>. [<a href="https://www.nature.com/articles/s41467-025-62865-w#additional-information:~:text=First%20Demonstration%20of,with%20Plaque%20Video">Demo</a>] [<a href="https://github.com/LeapLabTHU/UltraBot">GitHub</a>] 
   </li>
   <li> [2025.06] Two Papers are Accepted by ICCV 2025.
   </li>
   <li> [2025.02] Four Papers are Accepted by CVPR 2025 (with one <b><font color="red">highlight</font></b>).
   </li>
   <li> 	[2024.12] <a href="https://arxiv.org/pdf/2412.11228">Uni-AdaFocus</a> is Accepted by <b><font color="red">TPAMI</font></b>. Check it out on <a href="https://github.com/LeapLabTHU/Uni-AdaFocus">GitHub</a>.
   </li>
   <li> [2024.10] The <a href="https://link.springer.com/article/10.1007/s11263-024-02296-0">Journal Version of InfoPro</a> is Accepted by <b><font color="red">IJCV</font></b>.
   </li>
   <li> [2024.09] Two Papers are Accepted by NeurIPS 2024.
   </li>
   <li> [2024.07] One Paper is Accepted by ECCV 2024.
   </li>
   <li> 	[2024.05] <a href="https://arxiv.org/pdf/2405.08768">EfficientTrain++</a> is Accepted by <b><font color="red">TPAMI</font></b>. Check it out on <a href="https://github.com/LeapLabTHU/EfficientTrain">GitHub</a>.
   </li>
   <li> [2024.02] One Paper is Accepted by CVPR 2024.
   </li>
   <li> 	[2024.02] One Paper on <a href="https://arxiv.org/pdf/2403.06726">Long-Tailed Visual Recognition</a> is Accepted by <b><font color="red">TPAMI</font></b>.
   </li>
   <li>	[2023.11] One Paper is Accepted by <b><font color="red">IJCV</font></b>.
   </li> 
      <li> [2023.10] Awarded the National Scholarship. <br>
         (国家奖学金, <b><font color="red">4th time</font></b>, <b>Top 2% in Tsinghua University</b>)
      </li>
      <li> [2023.07] Five Papers are Accepted by ICCV 2023.
      </li>
      <li> [2022.10] Winning the <a href="https://ur.bytedance.com/scholarship">2022 ByteDance Scholarship</a>. <br>
         (字节跳动奖学金, <b><font color="red">10 PhD students in China</font></b>)
      </li>
      <li> [2022.09] Winning the <a href="https://www.msra.cn/zh-cn/news/features/2022-fellows">2022 Microsoft Research Asia Fellowship Award</a>. <br>
         (“微软学者”奖学金, <b><font color="red">12 PhD students in the Asia-Pacific region</font></b>)
      </li>
      <li> 	[2022.07] The <a href="https://arxiv.org/pdf/2201.03014.pdf">Journal Version of GFNet</a> is Accepted by <b><font color="red">TPAMI</font></b>.
      </li>
      <li>	[2022.03 & 2022.07] <a href="https://arxiv.org/pdf/2112.14238.pdf">AdaFocusV2</a> & <a href="https://arxiv.org/pdf/2209.13465.pdf">AdaFocusV3</a> are Accepted by CVPR 2022 & ECCV 2022, respectively.
      </li>
      <li>	[2021.12] Winning the <a href="https://mp.weixin.qq.com/s/1Qkc2mZ_MJ2hKNZZDT7RQA">2021 Baidu Scholarship</a>. <br>
         (百度奖学金, <b><font color="red">10 PhD students worldwide</font></b>)
      </li>
      <li> [2021.10] Awarded the National Scholarship. <br>
         (国家奖学金, <b><font color="red">3rd time</font></b>, <b>Top 2% in Tsinghua University</b>)
      </li>
      <li>	[2021.10] Winning the <a href="https://tc.ccf.org.cn/ccfcv/xgzy/timing/2021-05-04/697856.shtml">2021 CCF-CV Outstanding Young Researcher Award</a>. <br>
         (CCF-CV学术新锐奖, <b><font color="red">3 PhD or Master students in China</font></b>)
      </li>
      <li> 	[2021.09] Not All Images are Worth 16x16 Words! Our <a href="https://arxiv.org/pdf/2105.15075.pdf">Dynamic ViT (DVT)</a> is Accepted by NeurIPS 2021.
      </li> 
      <li>	[2021.09] Our <a href="https://arxiv.org/pdf/2102.04906.pdf">Survey on Dynamic Neural Networks</a> is Accepted by <b><font color="red">TPAMI</font></b>.
      </li> 
      <li>	[2021.07] <a href="https://arxiv.org/pdf/2105.03245.pdf">AdaFocus</a> is Accepted by ICCV 2021 for <b><font color="red">Oral</font> Presentation</b>.
      </li> 
      <li> 	[2021.05] Selected to be an <a href="http://cvpr2021.thecvf.com/node/184">Outstanding Reviewer</a> of CVPR 2021.
      </li> 
      <li>	[2021.03] Three Papers are Accepted by CVPR 2021 (with one <b><font color="red">Oral</font></b>).
      </li> 
      <li>	[2021.01] The <a href="https://arxiv.org/pdf/2007.10538.pdf">Journal Version of ISDA</a> is Accepted by <b><font color="red">TPAMI</font></b>.
      </li> 
      <li>	[2021.01] One Paper is Accepted by ICLR 2021.
      </li> 
   </ul>

<h2>Publications</h2>


<h3>2025</h3>
<ul>
   <li>
      <a href=''>Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception</a><br>
      <b>Yulin Wang</b>, Yang Yue (乐洋), Yang Yue (乐阳), Huanqian Wang, Haojun Jiang, Yizeng Han, Zanlin Ni, Yifan Pu, Minglei Shi, Rui Lu, Qisen Yang, Andrew Zhao, Zhuofan Xia, Shiji Song, Gao Huang <br>
      <i><b><font color="red">Nature Machine Intelligence</font> (5-year IF=31.8)</b>, 2025</i><br>
      [<a href="https://github.com/LeapLabTHU/AdaptiveNN">Code</a>]
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2412.11228'>Uni-AdaFocus: Spatial-temporal Dynamic Computation for Video Recognition</a><br>
      <b>Yulin Wang</b>, Haoji Zhang, Yang Yue, Shiji Song, Chao Deng, Junlan Feng, Gao Huang  <br>
      <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b><font color="red">TPAMI</font></b>), 2025</i><br>
      [<a href="https://github.com/LeapLabTHU/Uni-AdaFocus">Code</a>] [<a href="https://mp.weixin.qq.com/s/KzTwjGNhSdX9Lpk4MuG6TQ">机器之心</a>]
   </li>
   <li>
      <a href='https://www.nature.com/articles/s41467-025-62865-w'>Towards Expert-level Autonomous Carotid Ultrasonography with Large-scale Learning-based Robotic System</a><br>
      Haojun Jiang, Andrew Zhao, Qian Yang, Xiangjie Yan, Teng Wang, <b>Yulin Wang</b>, Ning Jia, Jiangshan Wang, Guokun Wu, Yang Yue, Shaqi Luo, Huanqian Wang, Ling Ren, Siming Chen, Pan Liu, Guocai Yao, Wenming Yang, Shiji Song, Xiang Li, Kunlun He, Gao Huang <br>
      <i><b><font color="red">Nature Communications</font> (5-year IF=17.2)</b>, 2025</i><br>
      [<a href="https://github.com/LeapLabTHU/UltraBot">Code</a>] [<a href="https://www.nature.com/articles/s41467-025-62865-w#additional-information:~:text=First%20Demonstration%20of,with%20Plaque%20Video">Demo</a>] [<a href="https://drive.google.com/file/d/1_bU8kv8a9vbtQgA_lz8t_HZ_dmJhDUN9/view?usp=sharing">Poster</a>] [<a href="https://www.bilibili.com/video/BV1GoWGzKEQu/?vd_source=17f8133aaca9f7f8e61c08b61e26d162">HiT Webinar Talk</a>] [<a href="https://www.tsinghua.edu.cn/info/1175/121057.htm">清华新闻 Tsinghua News</a>] [<a href="https://mp.weixin.qq.com/s/YJqVwwHct0YfVJLwJC9GwA">清华自动化新闻 Tsinghua DA News</a>] <br>
      [<a href="https://mp.weixin.qq.com/s/6EvXsRu4S6r53OsUY0rcSQ">北京智源报道 BAAI News</a>] [<a href="https://mp.weixin.qq.com/s/WqzrJ_DXyhiaQs5TVlaDEQ">医工学人报道 Medical & Engineering Innovators</a>] [<a href="https://mp.weixin.qq.com/s/SEsCNWsoPUPPZqOfBRf2QQ">CVer 报道</a>]
   </li>
   <li>
      <a href='https://arxiv.org/abs/2408.15026'>UltraSeP: Sequence-aware Pre-training for Echocardiography Probe Movement Guidance</a><br>
      Haojun Jiang, Teng Wang, Zhenguo Sun, <b>Yulin Wang</b>, Yang Yue, Yu Sun, Ning Jia, Meng Li, Shaqi Luo, Shiji Song, Gao Huang <br>
      <i>Pattern Recognition (<b>PR</b>), 2025</i><br>
   </li>
   <li>
      <a href='https://openaccess.thecvf.com/content/CVPR2025/papers/Yue_EchoWorld_Learning_Motion-Aware_World_Models_for_Echocardiography_Probe_Guidance_CVPR_2025_paper.pdf'>EchoWorld: Learning Motion-Aware World Models for Echocardiography Probe Guidance
      </a><br>
      Yang Yue*, <b>Yulin Wang*</b>, Haojun Jiang, Pan Liu, Shiji Song, Gao Huang  <br>
      <font color="black"><i> [*co-first author, supervising Yang Yue (junior PhD student)]</i></font><br>
      <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025</i><br>
      [<a href="https://github.com/LeapLabTHU/EchoWorld">Code</a>]
   </li>
   <li>
      <a href='https://openaccess.thecvf.com/content/CVPR2025/papers/Yue_CheXWorld_Exploring_Image_World_Modeling_for_Radiograph_Representation_Learning_CVPR_2025_paper.pdf'>CheXWorld: Exploring Image World Modeling for Radiograph Representation Learning
      </a><br>
      Yang Yue*, <b>Yulin Wang*</b>, Chenxin Tao, Pan Liu, Shiji Song, Gao Huang  <br>
      <font color="black"><i> [*co-first author, supervising Yang Yue (junior PhD student)]</i></font><br>
      <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025</i><br>
      [<a href="https://github.com/LeapLabTHU/CheXWorld">Code</a>]
   </li>
   <li>
      <a href='https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote_CVPR_2025_paper.pdf'>XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?
      </a><br>
      Fengxiang Wang, Hongzhen Wang, Zonghao Guo, Di Wang, <b>Yulin Wang</b>, Mingshuo Chen, Qiang Ma, Long Lan, Wenjing Yang, Jing Zhang, Zhiyuan Liu, Maosong Sun  <br>
      <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR <font color="red">Highlight</font></b>), 2025</i><br>
      [<a href="https://xlrs-bench.github.io/">Code</a>]
   </li>
   <li>
      <a href='https://openaccess.thecvf.com/content/CVPR2025/papers/Guo_Everything_to_the_Synthetic_Diffusion-driven_Test-time_Adaptation_via_Synthetic-Domain_Alignment_CVPR_2025_paper.pdf'>Everything to the Synthetic: Diffusion-driven Test-time Adaptation via Synthetic-Domain Alignment
      </a><br>
      Jiayi Guo, Junhao Zhao, Chaoqun Du, <b>Yulin Wang</b>, Chunjiang Ge, Zanlin Ni, Shiji Song, Humphrey Shi, Gao Huang <br>
      <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025</i><br>
      [<a href="https://github.com/SHI-Labs/Diffusion-Driven-Test-Time-Adaptation-via-Synthetic-Domain-Alignment">Code</a>]
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2505.21375'>GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution</a><br>
      Fengxiang Wang, Mingshuo Chen, Yueying Li, Di Wang, Haotian Wang, Zonghao Guo, Zefan Wang, Boqi Shan, Long Lan, <b>Yulin Wang†</b>, Hongzhen Wang†, Wenjing Yang†, Bo Du, Jing Zhang†  <br>
      <font color="black"><i> [†corresponding authors]</i></font><br>
      <i>Advances in Neural Information Processing Systems (<b>NeurIPS <font color="red">Spotlight</font></b>), 2025</i><br>
      [<a href="https://github.com/MiliLab/GeoLLaVA-8K">Code</a>]
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2503.10392'>RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing</a><br>
      Fengxiang Wang, <b>Yulin Wang</b>, Di Wang, Haotian Wang, Mingshuo Chen, Hongzhen Wang, Haiyan Zhao, Yangang Sun, Shuo Wang, Long Lan, Wenjing Yang, Jing Zhang <br>
      <i>Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2025</i><br>
      [<a href="https://github.com/MiliLab/RoMA">Code</a>]
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2509.26231'>IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance</a><br>
      Jiayi Guo, Chuanhao Yan, Xingqian Xu, <b>Yulin Wang</b>, Kai Wang, Gao Huang, Humphrey Shi  <br>
      <i>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2025</i><br>
      [<a href="https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment">Code</a>] <br>
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2503.12450'>LazyMAR: Accelerating Masked Autoregressive Models via Feature Caching</a><br>
      Feihong Yan, Qingyan Wei, Jiayi Tang, Jiajun Li, <b>Yulin Wang</b>, Xuming Hu, Huiqi Li, Linfeng Zhang  <br>
      <i>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2025</i><br>
      [<a href="https://github.com/feihongyan1/LazyMAR">Code</a>] <br>
   </li>
</ul>

<h3>2024</h3>
<ul>
   <li>
      <a href='https://arxiv.org/pdf/2405.08768'>EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training</a><br>
      <b>Yulin Wang</b>, Yang Yue, Rui Lu, Yizeng Han, Shiji Song, Gao Huang  <br>
      <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b><font color="red">TPAMI</font></b>), 2024</i><br>
      [<a href="https://github.com/LeapLabTHU/EfficientTrain">Code</a>] [<a href="https://mp.weixin.qq.com/s/GsFXuNpZAF98bc7uMvOBCA">机器之心</a>]
   </li>
   <li>
      <a href='https://link.springer.com/article/10.1007/s11263-024-02296-0'>InfoPro: Locally Supervised Deep Learning by Maximizing Information Propagation</a><br>
      <b>Yulin Wang</b>, Zanlin Ni, Yifan Pu, Cai Zhou, Jixuan Ying, Shiji Song, Gao Huang  <br>
      <i>International Journal of Computer Vision (<b><font color="red">IJCV</font></b>), 2024</i><br>
      [<a href="https://github.com/blackfeather-wang/InfoPro-Pytorch">Code</a>] 
   </li>
      <li>
         <a href='https://arxiv.org/pdf/2403.06726'>Probabilistic Contrastive Learning for Long-Tailed Visual Recognition</a><br>
         Chaoqun Du, <b>Yulin Wang</b>, Shiji Song, Gao Huang  <br>
			<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b><font color="red">TPAMI</font></b>), 2024</i><br>
         [<a href="https://github.com/LeapLabTHU/ProCo">Code</a>] [<a href="https://mp.weixin.qq.com/s/qjWdQRiax8NOOan-e1r0Ww">机器之心</a>]
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2411.02359'>DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution</a><br>
      Yang Yue*, <b>Yulin Wang*</b>, Bingyi Kang, Yizeng Han, Shenzhi Wang, Shiji Song, Jiashi Feng, Gao Huang  <br>
      <font color="black"><i> [*co-first author, supervising Yang Yue (junior PhD student)]</i></font><br>
      <i>Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2024</i><br>
      [<a href="https://github.com/yueyang130/DeeR-VLA">Code</a>] [<a href="https://mp.weixin.qq.com/s/drJ-dRAHL3uGW2EH6tBd9g">量子位</a>]
</li>
   <li>
      <a href='https://arxiv.org/pdf/2411.06959'>ENAT: Rethinking Spatial-temporal Interactions in Token-based Image Synthesis</a><br>
      Zanlin Ni*, <b>Yulin Wang*</b>, Renping Zhou, Yizeng Han, Jiayi Guo, Zhiyuan Liu, Yuan Yao, Gao Huang  <br>
      <font color="black"><i> [*co-first author, supervising Zanlin Ni (junior PhD student)]</i></font><br>
      <i>Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2024</i><br>
      [<a href="https://github.com/LeapLabTHU/ENAT">Code</a>]
</li>
   <li>
      <a href='https://arxiv.org/pdf/2409.00342'>AdaNAT: Exploring Adaptive Policy for Token-Based Image Generation
      </a><br>
      Zanlin Ni*, <b>Yulin Wang*</b>, Renping Zhou, Rui Lu, Jiayi Guo, Jinyi Hu, Zhiyuan Liu, Yuan Yao, Gao Huang  <br>
      <font color="black"><i> [*co-first author, supervising Zanlin Ni (junior PhD student)]</i></font><br>
      <i>European Conference on Computer Vision (<b>ECCV</b>), 2024</i><br>
      [<a href="https://github.com/LeapLabTHU/AdaNAT">Code</a>] [<a href="https://mp.weixin.qq.com/s/wTcVBpAb97Kq1cEVbOMNbA">机器之心</a>]
</li>
   <li>
      <a href='https://arxiv.org/pdf/2406.05478'>Revisiting Non-Autoregressive Transformers for Efficient Image Synthesis
      </a><br>
      Zanlin Ni*, <b>Yulin Wang*</b>, Renping Zhou, Jiayi Guo, Jinyi Hu, Zhiyuan Liu, Shiji Song, Yuan Yao, Gao Huang  <br>
      <font color="black"><i> [*co-first author, supervising Zanlin Ni (junior PhD student)]</i></font><br>
      <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024</i><br>
      [<a href="https://github.com/LeapLabTHU/ImprovedNAT">Code</a>]
</li>
<li>
   <a href='https://arxiv.org/pdf/2407.12622'>Rethinking the Architecture Design for Efficient Generic Event Boundary Detection</a><br>
   Ziwei Zheng, Zechuan Zhang, <b>Yulin Wang</b>, Shiji Song, Gao Huang, Le Yang<br>
   <i>ACM International Conference on Multimedia (<b>ACM MM</b>), 2024</i><br>
   [<a href="https://github.com/Ziwei-Zheng/EfficientGEBD">Code</a>]<br>
</li>
</ul>

<h3>2023</h3>
<ul>
      <li>
         <a href='https://arxiv.org/pdf/2201.03014.pdf'>Glance and Focus Networks for Dynamic Visual Recognition</a><br>
         Gao Huang*, <b>Yulin Wang*</b>, Kangchen Lv, Haojun Jiang, Wenhui Huang, Pengfei Qi, Shiji Song  <br>
         <font color="red"><i> [<b>*co-first author</b> with my advisor]</i></font><br>
			<i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b><font color="red">TPAMI</font></b>), 2023</i><br>
         [<a href="https://github.com/blackfeather-wang/GFNet-Pytorch">Code</a>] [<a href="https://zhuanlan.zhihu.com/p/266306870">知乎 (on GFNet)</a>] <br>
   </li>
   <li>
      <a href='https://link.springer.com/article/10.1007/s11263-023-01944-1'>Adapting Across Domains via Target-Oriented Transferable Semantic Augmentation Under Prototype Constraint</a><br>
      Mixue Xie, Shuang Li, Kaixiong Gong, <b>Yulin Wang</b>, Gao Huang<br>
      <i>International Journal of Computer Vision (<b><font color="red">IJCV</font></b>), 2023</i><br>
      [<a href="https://github.com/BIT-DA/TTSA">Code</a>] 
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2309.00399.pdf'>Fine-grained Recognition with Learnable Semantic Data Augmentation</a><br>
      Yifan Pu, Yizeng Han, <b>Yulin Wang</b>, Junlan Feng, Chao Deng, Gao Huang <br>
      <i>IEEE Transactions on Image Processing (<b>TIP</b>), 2023</i><br>
   </li>
   <li>
      <a href='https://ieeexplore.ieee.org/document/10155270'>Dynamic Spatial Focus for Efficient Compressed Video Action Recognition</a><br>
      Ziwei Zheng, Le Yang, <b>Yulin Wang</b>, Miao Zhang, Lijun He, Gao Huang, Fan Li <br>
      <i>IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>), 2023</i><br>
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2308.13998.pdf'>Computation-efficient Deep Learning for Computer Vision: A Survey</a><br>
      <b>Yulin Wang</b>, Yizeng Han, Chaofei Wang, Shiji Song, Qi Tian, Gao Huang <br>
      <i>Cybernetics and Intelligence (sponsored by the Department of Automation, Tsinghua University -- 清华自动化主办), 2023</i><br>
   </li>
      <li>
         <a href='https://arxiv.org/pdf/2211.09703.pdf'>EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones</a><br>
         <b>Yulin Wang</b>, Yang Yue, Rui Lu, Tianjiao Liu, Zhao Zhong, Shiji Song, Gao Huang  <br>
			<i>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023</i><br>
         [<a href="https://github.com/LeapLabTHU/EfficientTrain">Code</a>] <br>
   </li>
      <li>
         <a href='https://arxiv.org/pdf/2212.04129.pdf'>Deep Incubation: Training Large Models by Divide-and-Conquering</a><br>
         Zanlin Ni*, <b>Yulin Wang*</b>, Jiangwei Yu, Haojun Jiang, Yue Cao, Gao Huang  <br>
         <font color="black"><i> [*co-first author, supervising Zanlin Ni (junior PhD student)]</i></font><br>
			<i>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023</i><br>
         [<a href="https://github.com/LeapLabTHU/Deep-Incubation">Code</a>] <br>
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2306.11248.pdf'>Dynamic Perceiver for Efficient Visual Recognition</a><br>
      Yizeng Han, Dongchen Han, Zeyu Liu, <b>Yulin Wang</b>, Xuran Pan, Yifan Pu, Chao Deng, Junlan Feng, Shiji Song, Gao Huang  <br>
      <i>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023</i><br>
      [<a href="https://github.com/LeapLabTHU/Dynamic_Perceiver">Code</a>] <br>
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2303.07820.pdf'>Adaptive Rotated Convolution for Rotated Object Detection</a><br>
      Yifan Pu, Yiru Wang, Zhuofan Xia, Yizeng Han, <b>Yulin Wang</b>, Weihao Gan, Zidong Wang, Shiji Song, Gao Huang  <br>
      <i>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023</i><br>
      [<a href="https://github.com/LeapLabTHU/ARC">Code</a>] 
   </li>
   <li>
      <a href='https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Borrowing_Knowledge_From_Pre-trained_Language_Model_A_New_Data-efficient_Visual_ICCV_2023_paper.pdf'>Borrowing Knowledge From Pre-trained Language Model: A New Data-efficient Visual Learning Paradigm</a><br>
      Wenxuan Ma, Shuang Li, Jinming Zhang, Chi Harold Liu, Jingxuan Kang, <b>Yulin Wang</b>, Gao Huang  <br>
      <i>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>), 2023</i><br>
      [<a href="https://github.com/BIT-DA/BorLan">Code</a>] 
   </li>
</ul>
<h3>2022</h3>
<ul>
   <li>
      <a href='https://arxiv.org/pdf/2007.10538.pdf'>Regularizing Deep Networks with Semantic Data Augmentation</a><br>
      <b>Yulin Wang</b>, Gao Huang, Shiji Song, Xuran Pan, Yitong Xia, Cheng Wu<br>
      <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b><font color="red">TPAMI</font></b>), 2022</i><br>
      [<a href="https://github.com/blackfeather-wang/ISDA-for-Deep-Networks">Code</a>] [<a href="https://zhuanlan.zhihu.com/p/344953635">知乎</a>] [<a href="https://mp.weixin.qq.com/s/6pgrQ_UcfJK57hq9jFT17g">新智元</a>] [<a href="https://mp.weixin.qq.com/s/HzDbxsQpvlk-cDAVOd4qxQ">AI科技评论</a>] <br>
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2102.04906.pdf'>Dynamic Neural Networks: A Survey</a><br>
      Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, <b>Yulin Wang</b><br>
      <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b><font color="red">TPAMI</font></b>), 2022</i><br>
      [<a href="https://mp.weixin.qq.com/s/TG_HBAR7Jrec4X02sxNGEA">智源社区</a>] [<a href="https://jmq.h5.xeknow.com/s/2H6ZSj">机器之心-在线讲座</a>] [<a href="https://www.bilibili.com/video/BV19B4y1A7Wy?from=search&seid=12254026542403915477">Bilibili</a>]<br>
   </li>
   <li>
      <a href='https://www.sciopen.com/article/10.26599/AIR.2022.9150011'>Meta-Semi: A Meta-Learning Approach for Semi-Supervised Learning</a><br>
      <b>Yulin Wang</b>, Jiayi Guo, Jiangshan Wang, Cheng Wu, Shiji Song, Gao Huang <br>
      <i>CAAI Artificial Intelligence Research (sponsored by CAAI -- 中国人工智能学会主办), 2022</i><br>
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2209.13465.pdf'>AdaFocus V3: On Unified Spatial-temporal Dynamic Video Recognition</a><br>
      <b>Yulin Wang</b>, Yang Yue, Xinhong Xu, Ali Hassani, Victor Kulikov, Nikita Orlov, Shiji Song, Humphrey Shi, Gao Huang<br>
      <i>European Conference on Computer Vision (<b>ECCV</b>), 2022</i><br>
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2112.14238.pdf'>AdaFocus V2: End-to-End Training of Spatial Dynamic Networks for Video Recognition</a><br>
      <b>Yulin Wang</b>, Yang Yue, Yuanze Lin, Haojun Jiang, Zihang Lai, Victor Kulikov, Nikita Orlov, Humphrey Shi, Gao Huang<br>
      <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022</i><br>
      [<a href="https://github.com/LeapLabTHU/AdaFocusV2">Code</a>]<br>
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2208.01195.pdf'>Making the Best of Both Worlds: A Domain-Oriented Transformer for Unsupervised Domain Adaptation</a><br>
      Wenxuan Ma, Jinming Zhang, Shuang Li, Chi Harold Liu, <b>Yulin Wang</b>, Wei Li<br>
      <i>ACM International Conference on Multimedia (<b>ACM MM</b>), 2022</i><br>
      [<a href="https://github.com/BIT-DA/Domain-Oriented-Transformer">Code</a>]<br>
   </li>
   
</ul>
<h3>2021</h3>
<ul>
   <li>
      <a href='https://arxiv.org/pdf/2105.15075.pdf'>Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition</a><br>
      <b>Yulin Wang</b>, Rui Huang, Shiji Song, Zeyi Huang, Gao Huang<br>
      <i>Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2021</i><br>
      [<a href="https://github.com/blackfeather-wang/Dynamic-Vision-Transformer">Code</a>] [<a href="https://zhuanlan.zhihu.com/p/377961269">知乎</a>] [<a href="https://mp.weixin.qq.com/s/1dXwhYWtj7i0XwCc05icQw">量子位</a>] [<a href="https://mp.weixin.qq.com/s/0PyclQ6K7CfM1Mmp1WTdGQ">AI科技评论</a>]<br>   
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2105.03245.pdf'>Adaptive Focus for Efficient Video Recognition</a><br>
      <b>Yulin Wang</b>, Zhaoxi Chen, Haojun Jiang, Shiji Song, Yizeng Han, Gao Huang<br>
      <i>IEEE/CVF International Conference on Computer Vision (<b>ICCV <font color="red">Oral</font></b>), 2021</i><br>
      [<a href="https://github.com/blackfeather-wang/AdaFocus">Code</a>] [<a href="https://drive.google.com/file/d/1cJ1ezpQ0bXDq08ajNln94kTJFgspRaKv/view?usp=sharing">Poster</a>] [<a href="https://zhuanlan.zhihu.com/p/416704427">知乎</a>] [<a href="https://www.bilibili.com/video/BV1vb4y1a7sD/">Bilibili</a>]<br>
   </li>
   <li>
      <a href='https://openreview.net/pdf?id=fAbkE6ant2'>Revisiting Locally Supervised Learning: An Alternative to End-to-end Training</a><br>
      <b>Yulin Wang</b>, Zanlin Ni, Shiji Song, Le Yang, Gao Huang<br>
      <i>International Conference on Learning Representations (<b>ICLR</b>), 2021</i><br>
      [<a href="https://github.com/blackfeather-wang/InfoPro-Pytorch">Code</a>] [<a href="https://drive.google.com/file/d/1tk85yiNckZWH0MOa65rES_MsDO78RmVC/view?usp=sharing">Poster</a>] [<a href="https://zhuanlan.zhihu.com/p/344288699">知乎</a>]  [<a href="https://mp.weixin.qq.com/s/UzlgbBtH0TKvBiueMh4qxQ">PaperWeekly</a>] [<a href="https://www.bilibili.com/video/BV1KQ4y197L4?share_source=copy_web">Bilibili</a>]<br>
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2103.12562.pdf'>Transferable Semantic Augmentation for Domain Adaptation</a><br>
      Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, <b>Yulin Wang</b>, Wei Li<br>
      <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR <font color="red">Oral</font></b>), 2021</i><br>
      [<a href="https://github.com/BIT-DA/TSA">Code</a>] <br>
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2103.12579.pdf'>MetaSAug: Meta Semantic Augmentation for Long-Tailed Visual Recognition</a><br>
      Shuang Li, Kaixiong Gong, Chi Harold Liu, <b>Yulin Wang</b>, Feng Qiao, Xinjing Cheng<br>
      <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021</i><br>
      [<a href="https://github.com/BIT-DA/MetaSAug">Code</a>] <br>
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2104.04382.pdf'>CondenseNet V2: Sparse Feature Reactivation for Deep Networks</a><br>
      Le Yang, Haojun Jiang, Ruojin Cai, <b>Yulin Wang</b>, Shiji Song, Gao Huang, Qi Tian<br>
      <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021</i><br>
      [<a href="https://github.com/jianghaojun/CondenseNetV2">Code</a>] <br>
   </li>

</ul>
<h3>2020</h3>
<ul>
   <li>
      <a href='https://drive.google.com/file/d/1Ys1xR4e8O579sspbgxS-Sj92ZI-wZ8tF/view'>Collaborative Learning with Corrupted Labels</a><br>
      <b>Yulin Wang</b>, Rui Huang, Gao Huang, Shiji Song, Cheng Wu<br>
      <i>Neural Networks (<b>NN</b>), 2020</i><br>
   </li>
   <li>
      <a href='https://arxiv.org/pdf/2010.05300.pdf'>Glance and Focus: A Dynamic Approach to Reducing Spatial Redundancy in Image Classification</a><br>
      <b>Yulin Wang</b>, Kangchen Lv, Rui Huang, Shiji Song, Le Yang, Gao Huang<br>
      <i>Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2020</i><br>
      [<a href="https://github.com/blackfeather-wang/GFNet-Pytorch">Code</a>] [<a href="https://drive.google.com/file/d/19N9ermvLGwx_Nx3GtE989IAL_V42LivS/view?usp=sharing">Poster</a>] [<a href="https://zhuanlan.zhihu.com/p/266306870">知乎</a>] [<a href="https://mp.weixin.qq.com/s/zLa_y3t1IGrEN2763qUcPA">AI科技评论</a>]<br>   
   </li>
</ul>
<h3>2019</h3>
<ul>
   <li>
      <a href='https://www.tandfonline.com/doi/abs/10.1080/00207543.2018.1543967?journalCode=tprs20'>Logistics-aware Manufacturing Service Collaboration Optimisation towards Industrial Internet Platform</a><br>
      <b>Yulin Wang</b>, Yongping Zhang, Fei Tao, Tingyu Chen, Ying Cheng, Shunkun Yang<br>
      <i>International Journal of Production Research (<b>IJPR</b>), 2019</i><br>
   </li>
   <li>
      <a href='https://arxiv.org/pdf/1909.12220.pdf'>Implicit Semantic Data Augmentation for Deep Networks</a><br>
      <b>Yulin Wang</b>, Xuran Pan, Shiji Song, Hong Zhang, Cheng Wu, Gao Huang<br>
      <i>Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2019</i><br>
      [<a href="https://github.com/blackfeather-wang/ISDA-for-Deep-Networks">Code</a>] [<a href="https://github.com/blackfeather-wang/ISDA-for-Deep-Networks/blob/master/Poster/NeurIPS%20poster%20v5.pdf">Poster</a>]<br>
    </li>
</ul>



<h2>Selected Awards and Honors</h2>
<ul>
   <li><a href="https://m.csig.org.cn/23/202411/52239.html">First Prize of Natural Science Award (3/5)</a>, China Society of Image and Graphics (CSIG), 2024  <br>
      (<b><font color="red">中国图象图形学学会自然科学一等奖</font></b>)</li>
   <li>National Scholarship, Ministry of Education of China, 2023  <br>
      (国家奖学金, <b><font color="red">4th time</font></b>, <b>Top 2% in Tsinghua University</b>)</li>
 <li><a href="https://ur.bytedance.com/scholarship">ByteDance Scholarship</a>, ByteDance Ltd., 2022 <br>
   (字节跳动奖学金, <b><font color="red">10 PhD students in China</font></b>)</li>
 <li><a href="https://www.msra.cn/zh-cn/news/features/2022-fellows">Microsoft Research Asia Fellowship Award</a>, Microsoft Research Asia, 2022 <br>
   (“微软学者”奖学金, <b><font color="red">12 PhD students in the Asia-Pacific region</font></b>)</li>
 <li>“Li Yanda” Scholarship, Tsinghua University, 2022  <br>
    (李衍达励学基金, <b>4 PhD students in the Department of Automation, Tsinghua University</b>)</li>
 <li><a href="https://mp.weixin.qq.com/s/1Qkc2mZ_MJ2hKNZZDT7RQA">Baidu Scholarship</a>, Baidu Inc., 2021  <br>
   (百度奖学金, <b><font color="red">10 PhD students worldwide</font></b>)</li>
 <li><a href="https://tc.ccf.org.cn/ccfcv/xgzy/timing/2021-05-04/697856.shtml">CCF-CV Outstanding Young Researcher Award</a>, China Computer Federation (CCF), 2021  <br>
   (CCF-CV学术新锐奖, <b><font color="red">3 PhD or Master students in China</font></b>)</li>
 <li>National Scholarship, Ministry of Education of China, 2021  <br>
   (国家奖学金, <b><font color="red">3rd time</font></b>, <b>Top 2% in Tsinghua University</b>)</li>
    <li>Outstanding Oral Presentation, Doctoral Students Forum, Tsinghua University, 2021</li>
    <li>Travel Award, NeurIPS, 2019</li>
    <li>“Shen Yuan” Medal, Beihang University, 2018  <br>
      (沈元奖章，<b><font color="red">Top 10 of 18,000+ undergraduate students in Beihang University</font></b>)</li>
  <li>National Scholarship, Ministry of Education of China, 2018  <br>
   (国家奖学金, <b><font color="red">2nd time</font></b>, <b>Top 2% in Beihang University</b>)</li>
  <li>National Scholarship, Ministry of Education of China, 2017  <br>
   (国家奖学金, <b><font color="red">1st time</font></b>, <b>Top 2% in Beihang University</b>)</li>
    <li>“Gong Xin” Innovation Scholarship, Ministry of Industry and Information Technology of China, 2017  <br>
      (工信部创新奖学金, <b>Top 1/231 in Beihang University</b>)</li>
    <li>First Prize in the “Zhou Peiyuan” Mechanics Competition for Undergraduate Students, 2017  <br>
      (全国周培源大学生力学竞赛一等奖, <b>Top 0.3%</b>)</li>
    <li>First Prize in National Undergraduate Mathematical Contest in Modeling, 2017  <br>
      (高教社杯全国大学生数学建模竞赛一等奖, <b>Top 0.2%</b>)</li>
    <li>Scholarship for Outstanding Academic Performance, Beihang University, 2016-2019  <br>
      (<b>Top 5% in Beihang University</b>)</li>
 </ul>



<h2>Academic Service</h2>
<table>
<tbody><tr>
<td style="width:20px">
</td>
<td valign="middle">
  <div>
  - Reviewer for TPAMI, IJCV, TCYB, TNNLS, TCSVT, Pattern Recognition, TMLR, ...<br><br><br>
  - Reviewer for ICML, NeurIPS, ICLR, CVPR, ICCV, ECCV, AAAI, ...
  <li>
   <b><font color="red">Outstanding Reviewer</font></b>, CVPR, 2021
   </li>
  <!-- - Co-sponsor of the Special Interest Group on Dynamic Neural Networks, Beijing Academy of Artificial Intelligence (BAAI).<br> -->
  <!-- <li>
   <a href="https://littlepure2333.github.io/dynamic-neural-network">https://littlepure2333.github.io/dynamic-neural-network</a>
   </li> -->
   <!-- <li>
      Core members include more than 20 researchers from 8 universities. We have organized more than 30 academic reports and tutorials. The cumulative audience has exceeded 1,000.
      </li> -->
  </ul>
  </div>
</td>
</tr></tbody>
</table>


<center>
   <br><br><br>
   <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=500&t=tt&d=Skb__L3rRQTUS6LP2xGVpILlpIBy2ZE_lTYFxPjZvA0'></script>
</center>
   

<div id="footer">
	<div id="footer-text"></div>
</div>
Last update: 10/2025 by Yulin Wang.
</body></html>

